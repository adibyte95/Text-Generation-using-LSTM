{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing the dependencies \n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '(', ')', '*', ',', '-', '.', '0', '3', ':', ';', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‘', '’', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "# here we can see all the unique charecters in the book\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '?': 13, '-': 7, '[': 14, 'n': 30, '!': 2, '.': 8, 'b': 18, '_': 16, 'i': 25, 'h': 24, '’': 44, ']': 15, 'v': 38, 's': 35, '\\n': 0, 'p': 32, 'e': 21, 'm': 29, 'l': 28, 'c': 19, 'x': 40, 'd': 20, 'u': 37, 'f': 22, '*': 5, 'a': 17, 'y': 41, '‘': 43, 't': 36, '0': 9, '(': 3, '“': 45, 'o': 31, ';': 12, '3': 10, 'r': 34, 'j': 26, '”': 46, 'k': 27, 'w': 39, ',': 6, 'z': 42, ')': 4, 'g': 23, 'q': 33, ':': 11}\n"
     ]
    }
   ],
   "source": [
    "#here we can see all the unique charecter are mapped to a unique number\n",
    "print(char_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144515\n",
      "Total Vocab:  47\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  144415\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    #print('seq in : ', seq_in)\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    #print('seq out : ', seq_out)\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 47)                12079     \n",
      "=================================================================\n",
      "Total params: 276,271\n",
      "Trainable params: 276,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 3.1564 - acc: 0.1636 Epoch 00001: loss improved from inf to 3.15624, saving model to weights-improvement-01-3.1562.hdf5\n",
      "144415/144415 [==============================] - 1075s 7ms/step - loss: 3.1562 - acc: 0.1636\n",
      "Epoch 2/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 3.0573 - acc: 0.1730 Epoch 00002: loss improved from 3.15624 to 3.05703, saving model to weights-improvement-02-3.0570.hdf5\n",
      "144415/144415 [==============================] - 1058s 7ms/step - loss: 3.0570 - acc: 0.1730\n",
      "Epoch 3/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 3.0498 - acc: 0.1734 Epoch 00003: loss improved from 3.05703 to 3.04981, saving model to weights-improvement-03-3.0498.hdf5\n",
      "144415/144415 [==============================] - 1056s 7ms/step - loss: 3.0498 - acc: 0.1734\n",
      "Epoch 4/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 3.0418 - acc: 0.1736 Epoch 00004: loss improved from 3.04981 to 3.04186, saving model to weights-improvement-04-3.0419.hdf5\n",
      "144415/144415 [==============================] - 1059s 7ms/step - loss: 3.0419 - acc: 0.1734\n",
      "Epoch 5/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 3.0192 - acc: 0.1730 Epoch 00005: loss improved from 3.04186 to 3.01903, saving model to weights-improvement-05-3.0190.hdf5\n",
      "144415/144415 [==============================] - 1055s 7ms/step - loss: 3.0190 - acc: 0.1731\n",
      "Epoch 6/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 2.9519 - acc: 0.1860 Epoch 00006: loss improved from 3.01903 to 2.95208, saving model to weights-improvement-06-2.9521.hdf5\n",
      "144415/144415 [==============================] - 1055s 7ms/step - loss: 2.9521 - acc: 0.1860\n",
      "Epoch 7/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 2.9078 - acc: 0.2016 Epoch 00007: loss improved from 2.95208 to 2.90769, saving model to weights-improvement-07-2.9077.hdf5\n",
      "144415/144415 [==============================] - 1056s 7ms/step - loss: 2.9077 - acc: 0.2017\n",
      "Epoch 8/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 2.8736 - acc: 0.2150 Epoch 00008: loss improved from 2.90769 to 2.87326, saving model to weights-improvement-08-2.8733.hdf5\n",
      "144415/144415 [==============================] - 1108s 8ms/step - loss: 2.8733 - acc: 0.2151\n",
      "Epoch 9/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 2.8447 - acc: 0.2235 Epoch 00009: loss improved from 2.87326 to 2.84469, saving model to weights-improvement-09-2.8447.hdf5\n",
      "144415/144415 [==============================] - 1085s 8ms/step - loss: 2.8447 - acc: 0.2234\n",
      "Epoch 10/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 2.8263 - acc: 0.2277 Epoch 00010: loss improved from 2.84469 to 2.82649, saving model to weights-improvement-10-2.8265.hdf5\n",
      "144415/144415 [==============================] - 1069s 7ms/step - loss: 2.8265 - acc: 0.2276\n",
      "Epoch 11/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 2.8029 - acc: 0.2324 Epoch 00011: loss improved from 2.82649 to 2.80311, saving model to weights-improvement-11-2.8031.hdf5\n",
      "144415/144415 [==============================] - 1064s 7ms/step - loss: 2.8031 - acc: 0.2324\n",
      "Epoch 12/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 2.7804 - acc: 0.2376 Epoch 00012: loss improved from 2.80311 to 2.78070, saving model to weights-improvement-12-2.7807.hdf5\n",
      "144415/144415 [==============================] - 1287s 9ms/step - loss: 2.7807 - acc: 0.2375\n",
      "Epoch 13/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 2.7618 - acc: 0.2409 Epoch 00013: loss improved from 2.78070 to 2.76176, saving model to weights-improvement-13-2.7618.hdf5\n",
      "144415/144415 [==============================] - 1358s 9ms/step - loss: 2.7618 - acc: 0.2410\n",
      "Epoch 14/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 2.7401 - acc: 0.2450 Epoch 00014: loss improved from 2.76176 to 2.74016, saving model to weights-improvement-14-2.7402.hdf5\n",
      "144415/144415 [==============================] - 1320s 9ms/step - loss: 2.7402 - acc: 0.2450\n",
      "Epoch 15/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 2.7217 - acc: 0.2479 Epoch 00015: loss improved from 2.74016 to 2.72196, saving model to weights-improvement-15-2.7220.hdf5\n",
      "144415/144415 [==============================] - 1241s 9ms/step - loss: 2.7220 - acc: 0.2477\n",
      "Epoch 16/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 2.7026 - acc: 0.2513 Epoch 00016: loss improved from 2.72196 to 2.70253, saving model to weights-improvement-16-2.7025.hdf5\n",
      "144415/144415 [==============================] - 1290s 9ms/step - loss: 2.7025 - acc: 0.2512\n",
      "Epoch 17/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 2.6817 - acc: 0.2566 Epoch 00017: loss improved from 2.70253 to 2.68165, saving model to weights-improvement-17-2.6817.hdf5\n",
      "144415/144415 [==============================] - 1313s 9ms/step - loss: 2.6817 - acc: 0.2566\n",
      "Epoch 18/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 2.6635 - acc: 0.2594 Epoch 00018: loss improved from 2.68165 to 2.66362, saving model to weights-improvement-18-2.6636.hdf5\n",
      "144415/144415 [==============================] - 1190s 8ms/step - loss: 2.6636 - acc: 0.2594\n",
      "Epoch 19/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 2.6476 - acc: 0.2634 Epoch 00019: loss improved from 2.66362 to 2.64754, saving model to weights-improvement-19-2.6475.hdf5\n",
      "144415/144415 [==============================] - 1210s 8ms/step - loss: 2.6475 - acc: 0.2634\n",
      "Epoch 20/20\n",
      "144000/144415 [============================>.] - ETA: 3s - loss: 2.6334 - acc: 0.2649 Epoch 00020: loss improved from 2.64754 to 2.63337, saving model to weights-improvement-20-2.6334.hdf5\n",
      "144415/144415 [==============================] - 1081s 7ms/step - loss: 2.6334 - acc: 0.2648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251108d4a58>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally training the model\n",
    "model.fit(X, y, epochs=20, batch_size=2000, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      " 25000/144415 [====>.........................] - ETA: 14:58 - loss: 2.6162 - acc: 0.2676"
     ]
    }
   ],
   "source": [
    "# training the model for more 40 epochs \n",
    "model.fit(X, y, epochs=40, batch_size=2500, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('saved weights small network/weights-improvement-20-2.6334.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" *** start of this project gutenberg ebook alice’s adventures in wonderland ***\n",
      "\n",
      "alice’s adventures i \"\n",
      "o the  a  a  ao the  an the  an the  a  a  a  a  a  a  a  a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#generating the text \n",
    "\n",
    "\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "#print(dataX)\n",
    "start = 0\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we can see that the model performs very badly. an educated guess for this is that the model is too simple and is also trained for smaller epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training larger model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 100, 256)          264192    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 47)                12079     \n",
      "=================================================================\n",
      "Total params: 801,583\n",
      "Trainable params: 801,583\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 3.1091 - acc: 0.1666 Epoch 00001: loss improved from inf to 3.10913, saving model to weights-improvement-01-3.1091-bigger.hdf5\n",
      "144415/144415 [==============================] - 3209s 22ms/step - loss: 3.1091 - acc: 0.1666\n",
      "Epoch 2/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 3.0332 - acc: 0.1735 Epoch 00002: loss improved from 3.10913 to 3.03319, saving model to weights-improvement-02-3.0332-bigger.hdf5\n",
      "144415/144415 [==============================] - 3174s 22ms/step - loss: 3.0332 - acc: 0.1735\n",
      "Epoch 3/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 2.9270 - acc: 0.1947 Epoch 00003: loss improved from 3.03319 to 2.92689, saving model to weights-improvement-03-2.9269-bigger.hdf5\n",
      "144415/144415 [==============================] - 3175s 22ms/step - loss: 2.9269 - acc: 0.1947\n",
      "Epoch 4/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 2.8119 - acc: 0.2298 Epoch 00004: loss improved from 2.92689 to 2.81177, saving model to weights-improvement-04-2.8118-bigger.hdf5\n",
      "144415/144415 [==============================] - 3177s 22ms/step - loss: 2.8118 - acc: 0.2299\n",
      "Epoch 5/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 2.7228 - acc: 0.2488 Epoch 00005: loss improved from 2.81177 to 2.72281, saving model to weights-improvement-05-2.7228-bigger.hdf5\n",
      "144415/144415 [==============================] - 3176s 22ms/step - loss: 2.7228 - acc: 0.2488\n",
      "Epoch 6/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 2.6506 - acc: 0.2617 Epoch 00006: loss improved from 2.72281 to 2.65044, saving model to weights-improvement-06-2.6504-bigger.hdf5\n",
      "144415/144415 [==============================] - 3174s 22ms/step - loss: 2.6504 - acc: 0.2617\n",
      "Epoch 7/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 2.5767 - acc: 0.2765 Epoch 00007: loss improved from 2.65044 to 2.57659, saving model to weights-improvement-07-2.5766-bigger.hdf5\n",
      "144415/144415 [==============================] - 3173s 22ms/step - loss: 2.5766 - acc: 0.2765\n",
      "Epoch 8/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 2.5078 - acc: 0.2922 Epoch 00008: loss improved from 2.57659 to 2.50777, saving model to weights-improvement-08-2.5078-bigger.hdf5\n",
      "144415/144415 [==============================] - 3214s 22ms/step - loss: 2.5078 - acc: 0.2922\n",
      "Epoch 9/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 2.4479 - acc: 0.3052 Epoch 00009: loss improved from 2.50777 to 2.44798, saving model to weights-improvement-09-2.4480-bigger.hdf5\n",
      "144415/144415 [==============================] - 3140s 22ms/step - loss: 2.4480 - acc: 0.3052\n",
      "Epoch 10/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 2.3888 - acc: 0.3187 Epoch 00010: loss improved from 2.44798 to 2.38884, saving model to weights-improvement-10-2.3888-bigger.hdf5\n",
      "144415/144415 [==============================] - 3144s 22ms/step - loss: 2.3888 - acc: 0.3187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25f005b1b70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=10, batch_size=1300, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 2.3321 - acc: 0.3312 Epoch 00001: loss improved from 2.38884 to 2.33219, saving model to weights-improvement-01-2.3322-bigger.hdf5\n",
      "144415/144415 [==============================] - 3275s 23ms/step - loss: 2.3322 - acc: 0.3312\n",
      "Epoch 2/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 2.2832 - acc: 0.3449 Epoch 00002: loss improved from 2.33219 to 2.28324, saving model to weights-improvement-02-2.2832-bigger.hdf5\n",
      "144415/144415 [==============================] - 3333s 23ms/step - loss: 2.2832 - acc: 0.3450\n",
      "Epoch 3/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 2.2351 - acc: 0.3579 Epoch 00003: loss improved from 2.28324 to 2.23511, saving model to weights-improvement-03-2.2351-bigger.hdf5\n",
      "144415/144415 [==============================] - 3205s 22ms/step - loss: 2.2351 - acc: 0.3579\n",
      "Epoch 4/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 2.1945 - acc: 0.3696 Epoch 00004: loss improved from 2.23511 to 2.19446, saving model to weights-improvement-04-2.1945-bigger.hdf5\n",
      "144415/144415 [==============================] - 3310s 23ms/step - loss: 2.1945 - acc: 0.3696\n",
      "Epoch 5/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 2.1580 - acc: 0.3802 Epoch 00005: loss improved from 2.19446 to 2.15817, saving model to weights-improvement-05-2.1582-bigger.hdf5\n",
      "144415/144415 [==============================] - 3150s 22ms/step - loss: 2.1582 - acc: 0.3801\n",
      "Epoch 6/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 2.1217 - acc: 0.3895 Epoch 00006: loss improved from 2.15817 to 2.12190, saving model to weights-improvement-06-2.1219-bigger.hdf5\n",
      "144415/144415 [==============================] - 3139s 22ms/step - loss: 2.1219 - acc: 0.3895\n",
      "Epoch 7/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 2.0905 - acc: 0.3964 Epoch 00007: loss improved from 2.12190 to 2.09066, saving model to weights-improvement-07-2.0907-bigger.hdf5\n",
      "144415/144415 [==============================] - 3180s 22ms/step - loss: 2.0907 - acc: 0.3963\n",
      "Epoch 8/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 2.0630 - acc: 0.4049 Epoch 00008: loss improved from 2.09066 to 2.06302, saving model to weights-improvement-08-2.0630-bigger.hdf5\n",
      "144415/144415 [==============================] - 3342s 23ms/step - loss: 2.0630 - acc: 0.4049\n",
      "Epoch 9/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 2.0332 - acc: 0.4122 Epoch 00009: loss improved from 2.06302 to 2.03305, saving model to weights-improvement-09-2.0331-bigger.hdf5\n",
      "144415/144415 [==============================] - 3251s 23ms/step - loss: 2.0331 - acc: 0.4122\n",
      "Epoch 10/10\n",
      "144300/144415 [============================>.] - ETA: 2s - loss: 2.0036 - acc: 0.4207 Epoch 00010: loss improved from 2.03305 to 2.00345, saving model to weights-improvement-10-2.0035-bigger.hdf5\n",
      "144415/144415 [==============================] - 3210s 22ms/step - loss: 2.0035 - acc: 0.4207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25f6a59d978>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the model for another round of 10 epochs \n",
    "model.fit(X, y, epochs=10, batch_size=1300, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" do in an offended tone, ‘was, that\n",
      "the best thing to get us dry would be a caucus-race.’\n",
      "\n",
      "‘what is a \"\n",
      " larter ’ said the ming. ‘i dan to the thing the was to the thing the was to the thing the was a little said the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with the was oot a little sabbit with \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#generating the text\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that generally there are fewer spelling mistakes and the text looks more realistic, but is still quite nonsensical.\n",
    "try fitting a more complex network or try training this network for longer epochs in order to see considerable differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
